{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zrvpg5qKq2Xu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrvpg5qKq2Xu",
        "outputId": "7b9905df-6e41-4a6b-d205-c65c4ec552c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Found existing installation: gensim 4.3.3\n",
            "Uninstalling gensim-4.3.3:\n",
            "  Successfully uninstalled gensim-4.3.3\n",
            "Found existing installation: pandas 2.2.3\n",
            "Uninstalling pandas-2.2.3:\n",
            "  Successfully uninstalled pandas-2.2.3\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m177.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "Successfully installed numpy-2.2.6\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m300.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m242.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.6\n",
            "    Uninstalling numpy-2.2.6:\n",
            "      Successfully uninstalled numpy-2.2.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dask-cuda 25.2.0 requires pandas>=1.3, which is not installed.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, which is not installed.\n",
            "xarray 2025.3.1 requires pandas>=2.1, which is not installed.\n",
            "geopandas 1.0.1 requires pandas>=1.4.0, which is not installed.\n",
            "mlxtend 0.23.4 requires pandas>=0.24.2, which is not installed.\n",
            "bigframes 2.4.0 requires pandas>=1.5.3, which is not installed.\n",
            "pandas-gbq 0.28.1 requires pandas>=1.1.4, which is not installed.\n",
            "arviz 0.21.0 requires pandas>=1.5.0, which is not installed.\n",
            "cufflinks 0.17.3 requires pandas>=0.19.2, which is not installed.\n",
            "tensorflow-decision-forests 1.11.0 requires pandas, which is not installed.\n",
            "prophet 1.1.6 requires pandas>=1.0.4, which is not installed.\n",
            "shap 0.47.2 requires pandas, which is not installed.\n",
            "fastai 2.7.19 requires pandas, which is not installed.\n",
            "geemap 0.35.3 requires pandas, which is not installed.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, which is not installed.\n",
            "pymc 5.22.0 requires pandas>=0.24.0, which is not installed.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, which is not installed.\n",
            "dopamine-rl 4.1.2 requires pandas>=0.24.2, which is not installed.\n",
            "libpysal 4.13.0 requires pandas>=1.4, which is not installed.\n",
            "holoviews 1.20.2 requires pandas>=1.3, which is not installed.\n",
            "cmdstanpy 1.2.5 requires pandas, which is not installed.\n",
            "bokeh 3.7.3 requires pandas>=1.2, which is not installed.\n",
            "bqplot 0.12.44 requires pandas<3.0.0,>=1.0.0, which is not installed.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, which is not installed.\n",
            "datascience 0.17.6 requires pandas, which is not installed.\n",
            "datasets 2.14.4 requires pandas, which is not installed.\n",
            "statsmodels 0.14.4 requires pandas!=2.1.0,>=1.4, which is not installed.\n",
            "tsfresh 0.21.0 requires pandas>=0.25.0, which is not installed.\n",
            "yfinance 0.2.61 requires pandas>=1.3.0, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires pandas>=1.1.4, which is not installed.\n",
            "seaborn 0.13.2 requires pandas>=1.2, which is not installed.\n",
            "db-dtypes 1.4.3 requires pandas>=1.5.3, which is not installed.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m246.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.2.3\n"
          ]
        }
      ],
      "source": [
        "# Uninstall existing numpy, gensim, and pandas forcefully\n",
        "!pip uninstall numpy gensim pandas -y\n",
        "\n",
        "# Reinstall numpy first. Using --no-cache-dir can sometimes help prevent caching issues.\n",
        "!pip install numpy --upgrade --no-deps --no-cache-dir\n",
        "\n",
        "# Reinstall gensim. This should now use the newly installed numpy during its setup.\n",
        "!pip install gensim --upgrade --no-cache-dir\n",
        "\n",
        "# Reinstall pandas, ensuring it uses the newly installed numpy\n",
        "!pip install pandas --upgrade --no-cache-dir\n",
        "\n",
        "# After running these commands, **RESTART YOUR PYTHON KERNEL**\n",
        "# Then, run your notebook cells from the beginning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ueq6hq90swn4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueq6hq90swn4",
        "outputId": "6df7bcca-2be1-4959-e314-f39179758f9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyro-ppl in /usr/local/lib/python3.11/dist-packages (1.9.1)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl) (3.4.0)\n",
            "Requirement already satisfied: pyro-api>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl) (0.1.2)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->pyro-ppl) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->pyro-ppl) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install pyro-ppl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GCKnkDzrq0U3",
      "metadata": {
        "id": "GCKnkDzrq0U3"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c99dfebf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c99dfebf",
        "outputId": "5e707a14-f069-4306-ad46-7b15eb618047"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniconda/base/envs/bml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/jensparslov/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/jensparslov/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/jensparslov/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "from gensim.corpora import Dictionary\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.distributions import constraints\n",
        "import functools\n",
        "\n",
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "from pyro.infer import SVI, JitTraceEnum_ELBO, TraceEnum_ELBO\n",
        "from pyro.contrib.autoguide import AutoDiagonalNormal, AutoMultivariateNormal, AutoGuideList, AutoDelta\n",
        "from pyro.optim import ClippedAdam\n",
        "\n",
        "# Download required resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0b5587a9",
      "metadata": {
        "id": "0b5587a9"
      },
      "outputs": [],
      "source": [
        "# Initialize tools\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "# Define the preprocessing function\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())  # Tokenize and lowercase\n",
        "    tokens = [word for word in tokens if word.isalpha()]  # Remove punctuation and numbers\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Remove stop words\n",
        "    tokens = [stemmer.stem(word) for word in tokens]  # Stem the words\n",
        "    return tokens\n",
        "\n",
        "def add_mask_column(df, column_name, length, new_column_name='masked_list'):\n",
        "    def pad_and_create_mask(lst):\n",
        "        # Truncate if list is longer than the specified length\n",
        "        truncated = lst[:length]\n",
        "\n",
        "        # Create mask: 1s for actual values, 0s for padding\n",
        "        mask = [1] * len(truncated)\n",
        "        padding_length = max(0, length - len(truncated))\n",
        "        mask += [0] * padding_length\n",
        "\n",
        "        # Pad list with 0s if shorter than desired length\n",
        "        padded_lst = truncated + [0] * padding_length\n",
        "\n",
        "        return padded_lst, mask\n",
        "\n",
        "    # Apply padding and mask creation\n",
        "    result = df[column_name].apply(pad_and_create_mask)\n",
        "    df[column_name] = result.apply(lambda x: x[0])  # overwrite with padded/truncated list\n",
        "    df[new_column_name] = result.apply(lambda x: x[1])  # add mask column\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "d2ff8f56",
      "metadata": {
        "id": "d2ff8f56"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"bbc-news-data.csv\", sep='\\t', encoding='utf-8')\n",
        "df['content'] = df['content'].apply(preprocess_text)\n",
        "dictionary = Dictionary(df['content'])\n",
        "df['content_indexed'] = df['content'].apply(lambda tokens: dictionary.doc2idx(tokens))\n",
        "df['content_length'] = df['content_indexed'].apply(len)\n",
        "df = df[df['content_length'] >= 20]\n",
        "\n",
        "# Create a mask column for the dataframe\n",
        "df = add_mask_column(df, 'content_indexed', length=60, new_column_name=\"content_masked\")\n",
        "\n",
        "\n",
        "W = df[\"content_indexed\"]\n",
        "Mask = df[\"content_masked\"]\n",
        "\n",
        "# Convert to a LongTensor (2D tensor)\n",
        "tensor_W = torch.tensor(W.tolist(), dtype=torch.long).T\n",
        "tensor_Mask = torch.tensor(Mask.tolist(), dtype=torch.bool).T\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "fd5787a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd5787a7",
        "outputId": "42f91c83-f4be-49f9-f4f0-762812bcdd3b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1IAAAIjCAYAAAAJLyrXAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAS99JREFUeJzt3Qd8VFX6//FnQiodgiQgBFFQQJCqyMpioYMuCGsDNCoru4oNEJG1USyAYkMEdaWtAsquuisrKE1Q6QiIECOKGjrSSwop9/96jr+ZfyYkkBNC7szk8369xsnM3Mw9d3IG5zvnnOd6HMdxBAAAAABQaGGF3xQAAAAAoAhSAAAAAGCJIAUAAAAAlghSAAAAAGCJIAUAAAAAlghSAAAAAGCJIAUAAAAAlghSAAAAAGCJIAUAAAAAlghSAEqlCy64QO688063mxHyXnjhBbnwwgulTJky0qxZMwl111xzjbkEg2Bqqxs8Ho/cf//9bjcDQAAjSAEIetOmTTMfetauXZvv4/phsXHjxme9n08//VRGjBhx1s9TWnz++efy6KOPylVXXSVTp06V55577oy/M3fuXOnSpYvExsZKdHS0XHzxxfLII4/IgQMHSqTNoeJchYDly5eb98Dhw4cLtb1+WVG+fHkJVLbHAwC5hfvdAoBSIjk5WcLCwqyD1MSJEwlThbR48WLzGr/zzjsSGRl5xu01MI0fP16aNm0qw4YNk6pVq8o333wjr7/+usyePVsWLVokl1xyiQR6eAwWRWmrBo+RI0eagFS5cmUJdqF2PABKFkEKQKkUFRUlwebEiRNSrlw5CRb79u2TmJiYQoWoWbNmmRB1yy23yHvvvWemAnrph9xrr71WbrrpJhOswsPDA/Y1L8yxBopgaisABCKm9gEolfKukcrMzDTfTNevX99MKdOpZW3btpUFCxaYx3VbHY3yTpvyXnJ/4B4yZIjUrl3bhDQdOXnxxRfFcRy//aalpcmDDz4o1apVkwoVKsif/vQn2blzp3mu3CNd+rPet2XLFunTp49UqVLFtEd9++23pj269kjbGh8fL3ffffcp09+8z/HDDz9Iv379pFKlSnLeeefJk08+adq1fft26dGjh1SsWNE8hwaZwsjKypLRo0fLRRddZI5VX8u///3vkpGR4dtG96vT+fR18b5WOgWzIPra6zG+9dZbfiFKXXHFFWaEatOmTfKvf/3L3KfT1nTKWGpq6inPddttt5njyc7O9t03b948+eMf/2hCkb7u3bt3l82bN+c7De2nn36Sbt26me369u1rta/81h3p6/L0009LvXr1zOulfUSnPOZ+vXr16iUtWrTw+70bbrjBvG7//e9/ffetWrXK3KfHc7bya+uECRPk0ksvlbJly5q/R6tWrWTmzJm+/jR06FDzc926dX1/119++eWs26LHpVM6tY/qvq+++mr5+uuv8+3PP/74o28ESbe/6667TvnbFOZ9Vtjj+fjjj83UYP3b6Wszf/58v8ePHTsmDz/8sHkf6DbVq1eXjh07mtAPILQRpACEjCNHjsj+/ftPuWhIOhP9UKUf5nXkQ6eSPf7445KQkOD7MPTXv/7VfDhS//znP30XpaFEP6i9/PLL5sPgSy+9ZIKUfkgbPHiw3370A6B+WNUP6mPHjjUjNvqhviA6CqMfEnV90T333GPu03C3bds28wFSn+vWW281U9/0OfMGN6WjPDk5OTJmzBhp3bq1PPPMM/LKK6+Y4zn//PNNO/RDvk6tW7Zs2Rlfq7/85S/y1FNPmQ/+esz6off555837fDS10aDi36w9L5W7dq1y/f5tm7daqZaekNdfu644w7fGirvMWlI+9///ue3nb5Wn3zyifz5z3/2BTLdt77GGob0WDVIakDVYJr3Q7OGxM6dO5sPwxqEe/fubbWvvPR1176hz6XBSP9ePXv2NK+bPq+XvlYbN26Uo0ePmtv6d9QgoVMjv/zyS992+rPep+vOitvbb79twkejRo1M/9D3gxYI0ZDjDXsaHJW23/t31XB+tlNAtW/osWvg1L6ua5auu+46Wb169Snb33zzzSa8aJ/TnzWga1tt32eFOZ6vvvpK7rvvPtO3x40bJ+np6aZP5P7S4m9/+5tMmjTJ3P/GG2+Y95HuLykp6axeFwBBwAGAIDd16lRND6e9XHrppX6/U6dOHScxMdF3u2nTpk737t1Pu5+BAwea58rr448/Nvc/88wzfvf/+c9/djwej/Pjjz+a2+vWrTPbPfzww37b3Xnnneb+p59+2nef/qz33XbbbafsLzU19ZT7Zs2aZbZftmzZKc8xYMAA331ZWVlOrVq1TLvGjBnju//QoUNOTEyM32uSnw0bNpjn/Mtf/uJ3/yOPPGLuX7x4se8+fa5y5co5Z+J9/V5++eXTblexYkWnRYsW5uecnBzn/PPPd3r37u23zQcffOD3Ohw7dsypXLmyc8899/htt2fPHqdSpUp+92t79Xcfe+wxv20Luy919dVXm4vXP//5TycsLMz58ssv/X538uTJ5ne//vprc3vNmjXm9qeffmpuf/vtt+b2TTfd5LRu3dr3e3/605+c5s2bO2eiv6v99XTytrVHjx6nvE/yeuGFF8xz//zzz05hnKkP6Gtbv359p3Pnzubn3H28bt26TseOHU/pz3fffbffc9x4441ObGys77bN++x0x6P3R0ZG+t6/auPGjeb+CRMm+O7TfnSm1xpAaGJECkDI0Kl3OlqT93LZZZed8Xd1mpBO9dLREVtahEJHJPTb/Nx0qp9+HvNOw/JOCdJvuHN74IEHCnxu/bY7L/2220u/IddRtyuvvNLczm86kY4geWk7dbqWtqt///5+x6+jaDrSdaZjVXlH2vRYVd5Rm8LQ0QWlU7BORx/3jtjoFCwdrdP2HD9+3LfN+++/b0bZvNMg9e+voxs68pB7lFJfBx2dW7JkySn7uffee/1uF3Zf+ZkzZ440bNhQGjRo4Ld/HW1R3v03b97cjJh5RwR15KlWrVpmJE7/pjr6pX8zHSHR0atzQfvAjh07ZM2aNVJSNmzYYN5zOn1VR3m8r4+OALZv3968Hjqqd7r3hL4e+rvevlGU91lBOnToYKaweum/JTpqmvt9oq+bjtrt2rXL+vkBBDeCFICQoWtp9INP3ouu9TiTUaNGmQ/cWm67SZMmZlqerkUqjF9//VVq1qx5ShDQD9Dex73XOi1L12PkptPqCpJ3W3Xw4EF56KGHJC4uzoQqnYrk3U6nN+alUxRz03UlurZK14/kvf/QoUNnPFY9hrxt1nVC+oHSe6w2vK+bN1AVRB/P/Rrr1DhdC+NdQ6QhR8OOhh7v+jVvMNbgoq9T7otWrdOCGLlpIQsNMHkVZl/50f1rQM+7b+1nyrt/DXZt2rTxTePTaw0IGtJ0/dXKlSvNdET925+rIKXr0DTM6ftI1woOHDjwlHVKxc3790lMTDzlNfrHP/5h1pHl7dN5+7P3/e3tu0V5nxUk7768+8v9PtEpf999951Z+6avnU4TPtMXEgBCA1X7AEDErNHQIgP/+c9/zAds/RCn6yYmT57sN6JT0nKPPnnpuhAt26xhT9ew6Idf/dZe12fl/fZe5bd+p6A1PfmtscrP6cKDLW/gPF1w1Q/HOuKg63e8dBROF/h/8MEHZkRD1ytp2Mm99sj7eujaFw17eeWtAKhruvIri1+YfeVH96/BXNfN5Uc/fHtpaHr22WfNKKMGKV2np+FUCx3obQ3O6lwFKf076Fo1XYemozr//ve/zZofXQ+Xdw1ScfH+ffTEzQWdsDnveajOtu/aKMy+9P2of5OPPvrI/Nuhx6Lrsj788EPp2rVrsbcJQOAgSAHA/9HzFmkBB73oiIOGK/122RukCgoPderUkYULF54yYvL999/7Hvde6wfHn3/+2Xzj76VVyApLvwnX8ynpB1v9gOtVlCmJReE9Bt2fNwCpvXv3mhE977Ha0NEZvWh1tFdffTXfKX4zZsww19dff73f/fohVn9HQ5ZOtdOw453mqLzTsrR4hI5Ono0z7Ss/un8tIqHT1M4UPvXD+MmTJ00peK0w5w1M2g+9QUpfJ2+gOhe0qqGGQ71oW7Qgg4a74cOHm1HM4gzQuf8+Ol3ubP8+Xjbvs+I6nho1apiphHrRUUYtxKKvG0EKCG1M7QMAkVNKh+u34DoVKHeJau/5hDQw5KaVwXT6lVb7y01HtPSDmvfDlFaDU/otf25aXcz2G/K8375rlbWSoMea3/68Iy6nq0B4OhoKNSTq+pfcZcvVunXrzDf8OjKjldFy0w/8+jeaPn26GUXRsJObvub6IV0rweVXvfG3334rdBvPtK/86DYairQiXl46oqVrgbx0zVZERIQ5Vg31WmpbaaDSqX1Lly49Z6NR+b0H9DxTOgKofc372hX0Hiiqli1bmjClVQ1zrz8ryt/Hy+Z9drbHo30179RDDe061Tf3vx0AQhMjUgAgYj4w6jl19IOdfohdu3atOWeRnkPISx9TWlRCP6xpqNGyyFrWWsum61QsLafdtGlTM8VHpwnq+WW837rr72sQ0BCiH1p1NEM/HOt5ngr77biGAh2h0HUZ+uFWix3ovvTb95Kgx6brWfR8T/rhU0ufa4lqDRda1ltfh6LQ8zVpkQMd8dG1QHpb16JooYUpU6aY83rp30ODRm76zb8GXn3t9YNr3ql2+nppaerbb7/dbKt/L11/k5KSYgpjaBnxvAG4IGfaV350vzodUAOiFpbQ/emHbx2t1Ps/++wzU/xD6fmTtI9oaPKeQ0rp31sDl15sgpT2YS11n5f28/wKZHTq1MlMf9Q26qiXlu/W10bDsXeU0Pse0NdAX0v9e2hbT3fSYu2n+bVD32c6gqPTaPXLBg2OOhqsfVrDp75e+vfTaZQ2bN5nRTme3HQUWtfUaQl8fW/oFzA6Oq19ubDnZQMQxNwuGwgAxVX+XEtI50dLPJ+p/LmWLr/iiitMqWwtA96gQQPn2WefdU6ePOlXOvyBBx5wzjvvPFM+PPc/oVpme9CgQU7NmjWdiIgIU9JZSyvnLumsTpw4YUolV61a1SlfvrzTs2dPJzk52TxX7nLk3lLPv/322ynHs2PHDlPyWduqpZe1RPauXbsKLKGe9zkKKkmd3+uUn8zMTGfkyJGmPLUea+3atZ3hw4c76enphdrPmUqha8nrKlWqOFFRUU69evWcIUOG5Ps6eD3++OPmOHXbgixZssSU2NbXKzo62rnoootMOey1a9datfdM+8pbUlxpHxo7dqx5bfWY9NhatmxpXsMjR474bTt06FDz/Lp9bro/vf+nn35yCuN0pwIYPXp0vm198803nXbt2plS4tpOfY20PXnbqL+v5eC1rPuZSqF7S8rnd9Hn91q/fr3Tq1cv3771/XnzzTc7ixYtOmN/9r7/c7ejsO+z0x1PQSXkc//bkZGRYV4jPX1ChQoVTP/Rn994441C/JUABDuP/sftMAcApZmWgNby1++++64ZiQFQ/HifAShurJECgBKk62Ly0ilIWilOp3ABOHu8zwCUBNZIAUAJ0rVNWjxB1xJp6W09Wa9eBgwY4FcKG0DR8T4DUBKY2gcAJWjBggWmdLkWVNAqZXrCTy1IoIvd857TCEDR8D4DUBIIUgAAAABgiTVSAAAAAGCJIAUAAAAAlpgoLCI5OTmya9cuc8LBwpwQEwAAAEBo0pVPesLtmjVrmmqfBSFIiZgQRRUfAAAAAF7bt2+XWrVqSUEIUiJmJMr7YlWsWNHVtmRmZsrnn38unTp1koiICFfbApwJ/RXBhP6KYEFfRTDJDMH+evToUTPI4s0IBSFIaenC/5vOpyEqEIJU2bJlTTtCpTMidNFfEUzorwgW9FUEk8wQ7q9nWvJDsQkAAAAAsESQAgAAAABLBCkAAAAAsESQAgAAAABLBCkAAAAAsESQAgAAAABLBCkAAAAAsESQAgAAAABLBCkAAAAAsESQAgAAAABLBCkAAAAAsESQAgAAAABLBCkAAAAAsESQAgAAAABLBCkAAAAAsESQAgAAAABLBCkAAAAAsESQAgAAAABL4ba/gJKxceNGCQtzP+dWq1ZNEhIS3G4GAAAAEFAIUgFmx44d5rpdu3aSlpbmdnMkOqasJH+fRJgCAAAAciFIBZgDBw6Y66pdHpDsijVdbUvmge1yYO542b9/P0EKAAAAyIUgFaAiqp4v4dUucrsZAAAAAPLh/iIcAAAAAAgyBCkAAAAAsESQAgAAAABLBCkAAAAACLYgtXPnTunXr5/ExsZKTEyMNGnSRNauXet73HEceeqpp6RGjRrm8Q4dOsjWrVv9nuPgwYPSt29fqVixolSuXFn69+8vx48fd+FoAAAAAJQGrgapQ4cOyVVXXSUREREyb9482bJli4wfP16qVKni22bcuHHy2muvyeTJk2XVqlVSrlw56dy5s6Snp/u20RC1efNmWbBggcydO1eWLVsmAwYMcOmoAAAAAIQ6V8ufjx07VmrXri1Tp0713Ve3bl2/0ahXXnlFnnjiCenRo4e5b8aMGRIXFycff/yx3HrrrZKUlCTz58+XNWvWSKtWrcw2EyZMkG7dusmLL74oNWueei6mjIwMc/E6evSouc7MzDQXN+Xk5JjrqHCPOGUcV9viCfeYUUBtk9uvCwKTt1/QPxAM6K8IFvRVBJPMEOyvhT0Wj6NpxSWNGjUyo0s7duyQpUuXyvnnny/33Xef3HPPPebxbdu2yUUXXSTr16+XZs2a+X7v6quvNrdfffVVmTJligwZMsSMbnllZWVJdHS0zJkzR2688cZT9jtixAgZOXLkKffPnDlTypYte86OFwAAAEBgS01NlT59+siRI0fM0qGAHJHSoDRp0iQZPHiw/P3vfzejSg8++KBERkZKYmKi7Nmzx2ynI1C56W3vY3pdvXp1v8fDw8OlatWqvm3yGj58uNln7hEpHRnr1KnTaV+skqChcffu3TJsXoo4sf9/dM4NJ/duk70zHzNTJZs2bepqWxC439jolNqOHTuaKbpAIKO/IljQVxFMMkOwv3pnq52Jq0FKp4zpdLznnnvO3G7evLl89913Zj2UBqlzJSoqylzy0j++2x0gLOz3ZWsZWY442R5X26JtSEtLM21y+3VBYAuE9w5QWPRXBAv6KoJJRAj118Ieh6vFJrQSn07vy61hw4aSkpJifo6PjzfXe/fu9dtGb3sf0+t9+/b5Pa5T+7SSn3cbAAAAAChOrgYprdiXnJzsd98PP/wgderU8RWe0DC0aNEiv6E2rd7Xpk0bc1uvDx8+LOvWrfNts3jxYjPa1bp16xI7FgAAAAClh6tT+wYNGiR/+MMfzNS+m2++WVavXi1vvfWWuSiPxyMPP/ywPPPMM1K/fn0TrJ588klTia9nz56+EawuXbqYAhU6JVDnad5///2mol9+FfsAAAAAIKiD1OWXXy4fffSRKf4watQoE5S03LmeF8rr0UcflRMnTpjzQunIU9u2bU25c63K5/Xee++Z8NS+fXuznqd3797m3FMAAAAAEHJBSl1//fXmUhAdldKQpZeCaIU+LV0OAAAAACG/RgoAAAAAghFBCgAAAAAsEaQAAAAAwBJBCgAAAAAsEaQAAAAAwBJBCgAAAAAsEaQAAAAAwBJBCgAAAAAsEaQAAAAAwBJBCgAAAAAsEaQAAAAAwBJBCgAAAAAsEaQAAAAAwBJBCgAAAAAsEaQAAAAAwBJBCgAAAAAsEaQAAAAAwBJBCgAAAAAsEaQAAAAAwBJBCgAAAAAsEaQAAAAAwBJBCgAAAAAsEaQAAAAAwBJBCgAAAAAsEaQAAAAAwBJBCgAAAAAsEaQAAAAAwBJBCgAAAAAsEaQAAAAAwBJBCgAAAAAsEaQAAAAAwBJBCgAAAAAsEaQAAAAAwBJBCgAAAAAsEaQAAAAAwBJBCgAAAAAsEaQAAAAAwBJBCgAAAAAsEaQAAAAAwBJBCgAAAAAsEaQAAAAAwBJBCgAAAAAsEaQAAAAAwBJBCgAAAAAsEaQAAAAAwBJBCgAAAAAsEaQAAAAAwBJBCgAAAAAsEaQAAAAAwBJBCgAAAAAsEaQAAAAAwBJBCgAAAAAsEaQAAAAAwBJBCgAAAAAsEaQAAAAAwBJBCgAAAAAsEaQAAAAAwBJBCgAAAAAsEaQAAAAAwBJBCgAAAACCKUiNGDFCPB6P36VBgwa+x9PT02XgwIESGxsr5cuXl969e8vevXv9niMlJUW6d+8uZcuWlerVq8vQoUMlKyvLhaMBAAAAUFqEu92ASy+9VBYuXOi7HR7+/5s0aNAg+d///idz5syRSpUqyf333y+9evWSr7/+2jyenZ1tQlR8fLwsX75cdu/eLXfccYdERETIc88958rxAAAAAAh9rgcpDU4ahPI6cuSIvPPOOzJz5ky57rrrzH1Tp06Vhg0bysqVK+XKK6+Uzz//XLZs2WKCWFxcnDRr1kxGjx4tw4YNM6NdkZGRLhwRAAAAgFDnepDaunWr1KxZU6Kjo6VNmzby/PPPS0JCgqxbt04yMzOlQ4cOvm112p8+tmLFChOk9LpJkyYmRHl17txZ7r33Xtm8ebM0b948331mZGSYi9fRo0fNte5PL27Kyckx11HhHnHKOK62xRPukZiYGNMmt18XBCZvv6B/IBjQXxEs6KsIJpkh2F8LeyyuBqnWrVvLtGnT5JJLLjHT8kaOHCl//OMf5bvvvpM9e/aYEaXKlSv7/Y6GJn1M6XXuEOV93PtYQTSs6b7y0hEuXWsVCMZ2TdDJiy63oo7IDbNk586d5gIUZMGCBW43ASg0+iuCBX0VwWRBCPXX1NTUwA9SXbt29f182WWXmWBVp04d+eCDD8xIyLkyfPhwGTx4sN+IVO3ataVTp05SsWJFcdP69etNqBw2L0Wc2LqutuXk3m2yd+ZjsmzZMmnatKmrbUHgfmOj/3B27NjRrE0EAhn9FcGCvopgkhmC/dU7Wy3gp/blpqNPF198sfz444/mj3Hy5Ek5fPiw36iUVu3zrqnS69WrV/s9h7eqX37rrryioqLMJS/947vdAcLCfi+kmJHliJPtcbUt2oa0tDTTJrdfFwS2QHjvAIVFf0WwoK8imESEUH8t7HEE1Hmkjh8/Lj/99JPUqFFDWrZsaQ5i0aJFvseTk5NNuXNdS6X0etOmTbJv3z7fNpqIdVSpUaNGrhwDAAAAgNDn6ojUI488IjfccIOZzrdr1y55+umnpUyZMnLbbbeZcuf9+/c3U/CqVq1qwtEDDzxgwpMWmlA6FU8D0+233y7jxo0z66KeeOIJc+6p/EacAAAAACDog9SOHTtMaDpw4ICcd9550rZtW1PaXH9WL7/8splWpifi1Sp7WpHvjTfe8P2+hq65c+eaKn0asMqVKyeJiYkyatQoF48KAAAAQKhzNUjNnj37tI9rSfSJEyeaS0F0NOvTTz89B60DAAAAgCBYIwUAAAAAwYAgBQAAAACWCFIAAAAAYIkgBQAAAACWCFIAAAAAYIkgBQAAAACWCFIAAAAAYIkgBQAAAACWCFIAAAAAYIkgBQAAAACWCFIAAAAAYIkgBQAAAACWCFIAAAAAYCnc9hcA/C4lJUX2798vgaBatWqSkJDgdjMAAABKDYIUgkYgBZfdu3dL7z/fJBnpaRIIomPKSvL3SYQpAACAEkKQQtCEqEsaNJT0tFQJJLHXD5GI2NqutiHzwHY5MHe8CZkEKQAAgJJBkEJQ0JCgISoQgotK27ZWjnz5rmlLVHw9t5sDAACAEkaQQlAJlOCio0AAAAAovajaBwAAAACWCFIAAAAAYIkgBQAAAACWCFIAAAAAYIkgBQAAAACWCFIAAAAAYIkgBQAAAACWCFIAAAAAYIkgBQAAAACWCFIAAAAAYIkgBQAAAACWCFIAAAAAYIkgBQAAAACWCFIAAAAAYIkgBQAAAACWCFIAAAAAYIkgBQAAAACWCFIAAAAAYIkgBQAAAACWCFIAAAAAYIkgBQAAAACWwm1/AaVPUlKS200IiDYAAAAAXgQpFCj7+CERj0f69evndlMAAACAgEKQQoFyMo6LOI7EXj9EImJru9qWtG1r5ciX77raBgAAAMCLIIUz0hAVFV/P1TZkHtju6v4BAACA3Cg2AQAAAACWCFIAAAAAYIkgBQAAAACWCFIAAAAAYIkgBQAAAACWCFIAAAAAYIkgBQAAAACWCFIAAAAAYIkgBQAAAACWCFIAAAAAYIkgBQAAAACWCFIAAAAAYIkgBQAAAACWCFIAAAAAYIkgBQAAAACWCFIAAAAAYIkgBQAAAACWCFIAAAAAEKxBasyYMeLxeOThhx/23Zeeni4DBw6U2NhYKV++vPTu3Vv27t3r93spKSnSvXt3KVu2rFSvXl2GDh0qWVlZLhwBAAAAgNIiIILUmjVr5M0335TLLrvM7/5BgwbJJ598InPmzJGlS5fKrl27pFevXr7Hs7OzTYg6efKkLF++XKZPny7Tpk2Tp556yoWjAAAAAFBauB6kjh8/Ln379pW3335bqlSp4rv/yJEj8s4778hLL70k1113nbRs2VKmTp1qAtPKlSvNNp9//rls2bJF3n33XWnWrJl07dpVRo8eLRMnTjThCgAAAADOhXBxmU7d01GlDh06yDPPPOO7f926dZKZmWnu92rQoIEkJCTIihUr5MorrzTXTZo0kbi4ON82nTt3lnvvvVc2b94szZs3z3efGRkZ5uJ19OhRc63704ubcnJyzHVUuEecMo6rbcmKKCMxMTESHe6RSNoSsO3xhHtMW7TvlHT/9e7P7fcNUBj0VwQL+iqCSWYI9tfCHourQWr27NnyzTffmKl9ee3Zs0ciIyOlcuXKfvdraNLHvNvkDlHex72PFeT555+XkSNHnnK/jnDpWqtAMLZrgk5edLcRV/xBJPEP/3eDtgRue+qI3DBLdu7caS5uWLBggSv7BYqC/opgQV9FMFkQQv01NTU1sIPU9u3b5aGHHjIvenR0dInue/jw4TJ48GC/EanatWtLp06dpGLFiuKm9evXy+7du2XYvBRxYuu62pYTSV/KwfkTJK7PGImMu5C2BGh7Tu7dJntnPibLli2Tpk2blvg3Nvoe7tixo0RERJTovgFb9FcEC/oqgklmCPZX72y1gA1SOnVv37590qJFC7/iEfph8PXXX5fPPvvMrHM6fPiw36iUVu2Lj483P+v16tWr/Z7XW9XPu01+oqKizCUv/eO73QHCwn5ftpaR5YiT7XG1LemZ2ZKWlibptCWg26N9Rduifcet/hsI7x2gsOivCBb0VQSTiBDqr4U9DteKTbRv3142bdokGzZs8F1atWplCk94f9aDWLRoke93kpOTTbnzNm3amNt6rc+hgcxLE7GOKjVq1MiV4wIAAAAQ+lwbkapQoYI0btzY775y5cqZc0Z57+/fv7+Zgle1alUTjh544AETnrTQhNKpeBqYbr/9dhk3bpxZF/XEE0+YAhb5jTgBAAAAQEhU7Tudl19+2UxX0hPxapU9rcj3xhtv+B4vU6aMzJ0711Tp04ClQSwxMVFGjRrlarsBAAAAhLaAClJffPGF320tQqHnhNJLQerUqSOffvppCbQOAAAAAALkhLwAAAAAEGwIUgAAAABgiSAFAAAAAJYIUgAAAABgiSAFAAAAAJYIUgAAAABgiSAFAAAAAMF8HikARZeUlFTi+8zJyTHXGzduNCfPVtWqVZOEhIQSbwsAAEBJIkgBQS77+CERj0f69etX4vuOiYmRWbNmSbt27SQtLc3cFx1TVpK/TyJMAQCAkEaQAoJcTsZxEceR2OuHSERs7RLdd3S4x1zH9Rkj6VmOZB7YLgfmjpf9+/cTpAAAQEgjSAEhQkNUVHy9Et1nZBlHx8QkMu5CcbJ/D1UAAAClAcUmAAAAAMASQQoAAAAALBGkAAAAAMASQQoAAAAASiJIbdu2rSi/BgAAAAClN0jVq1dPrr32Wnn33XclPT29+FsFAAAAAKEWpL755hu57LLLZPDgwRIfHy9//etfZfXq1cXfOgAAAAAIlSDVrFkzefXVV2XXrl0yZcoU2b17t7Rt21YaN24sL730kvz222/F31IAAAAACIViE+Hh4dKrVy+ZM2eOjB07Vn788Ud55JFHpHbt2nLHHXeYgAUAAAAAoeasgtTatWvlvvvukxo1apiRKA1RP/30kyxYsMCMVvXo0aP4WgoAAAAAASK8KL+koWnq1KmSnJws3bp1kxkzZpjrsLDfc1ndunVl2rRpcsEFFxR3ewEAAAAgOIPUpEmT5O6775Y777zTjEblp3r16vLOO++cbfsAAAAAIDSC1NatW8+4TWRkpCQmJhbl6QEAAAAg9NZI6bQ+LTCRl943ffr04mgXAAAAAIRWkHr++eelWrVq+U7ne+6554qjXQAAAAAQWkEqJSXFFJTIq06dOuYxAAAAAAhlRQpSOvL07bffnnL/xo0bJTY2tjjaBQAAAAChFaRuu+02efDBB2XJkiWSnZ1tLosXL5aHHnpIbr311uJvJQAAAAAEe9W+0aNHyy+//CLt27eX8PDfnyInJ0fuuOMO1kgBAAAACHlFClJa2vz99983gUqn88XExEiTJk3MGikAAAAACHVFClJeF198sbkAAAAAQGlSpCCla6KmTZsmixYtkn379plpfbnpeikAAAAACFVFClJaVEKDVPfu3aVx48bi8XiKv2UAAAAAEEpBavbs2fLBBx9It27dir9FAAAAABCK5c+12ES9evWKvzUAAAAAEKpBasiQIfLqq6+K4zjF3yIAAAAACMWpfV999ZU5Ge+8efPk0ksvlYiICL/HP/zww+JqHwAAAACERpCqXLmy3HjjjcXfGgAAAAAI1SA1derU4m8JAAAAAITyGimVlZUlCxculDfffFOOHTtm7tu1a5ccP368ONsHAAAAAKExIvXrr79Kly5dJCUlRTIyMqRjx45SoUIFGTt2rLk9efLk4m8pAAAAAATziJSekLdVq1Zy6NAhiYmJ8d2v66YWLVpUnO0DAAAAgNAYkfryyy9l+fLl5nxSuV1wwQWyc+fO4mobAAAAAITOiFROTo5kZ2efcv+OHTvMFD8AAAAACGVFClKdOnWSV155xXfb4/GYIhNPP/20dOvWrTjbBwAAAAChMbVv/Pjx0rlzZ2nUqJGkp6dLnz59ZOvWrVKtWjWZNWtW8bcSAAAAAII9SNWqVUs2btwos2fPlm+//daMRvXv31/69u3rV3wCAAAAAEJReJF/MTxc+vXrV7ytAQAAAIBQDVIzZsw47eN33HFHUdsDAAAAAKEZpPQ8UrllZmZKamqqKYdetmxZghQAAACAkFakqn16It7cF10jlZycLG3btqXYBAAAAICQV6QglZ/69evLmDFjThmtAgAAAIBQU2xByluAYteuXcX5lAAAAAAQGmuk/vvf//rddhxHdu/eLa+//rpcddVVxdU2AAAAAAidINWzZ0+/2x6PR8477zy57rrrzMl6AQAAACCUFSlI5eTkFH9LAAAAAKA0rpECAAAAgNKgSCNSgwcPLvS2L730UlF2AQAAAAChFaTWr19vLnoi3ksuucTc98MPP0iZMmWkRYsWfmunAAAAACDUFGlq3w033CDt2rWTHTt2yDfffGMu27dvl2uvvVauv/56WbJkibksXrz4tM8zadIkueyyy6RixYrm0qZNG5k3b57v8fT0dBk4cKDExsZK+fLlpXfv3rJ3716/50hJSZHu3btL2bJlpXr16jJ06FDJysoqymEBAAAAwLkLUlqZ7/nnn5cqVar47tOfn3nmGauqfbVq1TIn8V23bp2sXbvWVP3r0aOHbN682Tw+aNAg+eSTT2TOnDmydOlSc46qXr16+X4/OzvbhKiTJ0/K8uXLZfr06TJt2jR56qmninJYAAAAAHDupvYdPXpUfvvtt1Pu1/uOHTtmNbKV27PPPmtGqVauXGlC1jvvvCMzZ840AUtNnTpVGjZsaB6/8sor5fPPP5ctW7bIwoULJS4uTpo1ayajR4+WYcOGyYgRIyQyMrIohwcAAAAAxR+kbrzxRrnrrrvM6NMVV1xh7lu1apWZVpd7xMiGji7pyNOJEyfMFD8dpdI1WB06dPBt06BBA0lISJAVK1aYIKXXTZo0MSHKq3PnznLvvfeaUa3mzZvnu6+MjAxzyR0Mle5PL27ylpaPCveIU8ZxtS1ZEWUkJiZGosM9EklbArY9brYlKszxu/aEe0xbtB+7/V4C8vL2SfomAh19FcEkMwT7a2GPxeM4jvUnr9TUVHnkkUdkypQpvh2Fh4dL//795YUXXpBy5coV+rk2bdpkgpOuh9J1UDoC1a1bN3OtYS134FEa3HQt1tixY2XAgAHy66+/ymeffebXNt3/p59+Kl27ds13nzpaNXLkyFPu133qWisAAAAApVNqaqr06dNHjhw5Yuo4FOuIlIaNN954w4Smn376ydx30UUXWQUoL636t2HDBtPQf/3rX5KYmGjWQ51Lw4cP9yvhriNStWvXlk6dOp32xSoJWg1x9+7dMmxeijixdV1ty4mkL+Xg/AkS12eMRMZdSFsCtD1utkVHoka3ypEn14ZJRo5HTu7dJntnPibLli2Tpk2blmhbgDPRL/4WLFggHTt2lIiICLebAxSIvopgkhmC/dU7W+1MihSkvPQDv160gp9O59HBLduS57qOqV69eubnli1bypo1a+TVV1+VW265xRSROHz4sFSuXNm3vVbti4+PNz/r9erVq/2ez1vVz7tNfqKioswlL/3ju90BwsJ+r/+RkeWIk+1u+fj0zGxJS0uTdNoS0O0JhLZoiMrI9ph+q23Rfuz2ewkoSCD8Ww8UBn0VwSQihPprYY+jSFX7Dhw4IO3bt5eLL77YTMPTMKV0at+QIUPkbOjaCp3Op6FKD2LRokW+x5KTk025c50KqPRapwbu27fPt40mYh1VatSo0Vm1AwAAAACKNUhpWXINORpqcq8p0lGk+fPnW02x0ylAv/zyiwlEevuLL76Qvn37SqVKlUww0yl4ek4qLT6ha6Y0PGmhCaVT8TQw3X777bJx40azVuqJJ54w557Kb8QJAAAAAIpDkab2adlxDS1aojy3+vXrm+IPhaUjSXfccYcZ0dLgpCfn1efVOZbq5ZdfNlOE9ES8OkqlFfl0bZZXmTJlZO7cuaZKnwYsXaOla6xGjRpVlMMCAAAAgHMXpLREeX7V7Q4ePGg1EqTniTqd6OhomThxorkUpE6dOqZCHwAAAAAE9NS+P/7xjzJjxgzfbS0woWubxo0bZ0qTAwAAAEAoK9KIlAYmLTaxdu1aU1nv0UcfNSfA1RGpr7/+uvhbCQAAAADBPiLVuHFj+eGHH6Rt27bSo0cPM9WvV69e5hxIej4pAAAAAAhl4UU56VaXLl1k8uTJ8vjjj5+bVgEAAABAKI1Iadnzb7/99ty0BgAAAABCdWpfv379zlhxDwAAAABCVZGKTWRlZcmUKVNk4cKF0rJlS3P+ptxeeuml4mofAAAAAAR3kNq2bZtccMEF8t1330mLFi3MfVp0IjcthQ4AAAAAocwqSNWvX192794tS5YsMbdvueUWee211yQuLu5ctQ8AAAAAgnuNlOM4frfnzZtnSp8DAAAAQGlSpGITBQUrAAAAACgNrIKUrn/KuwaKNVEAAAAASptw2xGoO++8U6Kioszt9PR0+dvf/nZK1b4PP/yweFsJAAAAAMEapBITE085nxQAAAAAlDZWQWrq1KnnriUAAAAAUBqKTQAAAABAaWQ1IgUAhZGUlCSBoFq1apKQkOB2MwAAQAgiSAEoNtnHD2kpz4BZPxkdU1aSv08iTAEAgGJHkAJQbHIyjmt5T4m9fohExNZ2tS2ZB7bLgbnjZf/+/QQpAABQ7AhSAIqdhqio+HpuNwMAAOCcodgEAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAARTkHr++efl8ssvlwoVKkj16tWlZ8+ekpyc7LdNenq6DBw4UGJjY6V8+fLSu3dv2bt3r982KSkp0r17dylbtqx5nqFDh0pWVlYJHw0AAACA0sLVILV06VITklauXCkLFiyQzMxM6dSpk5w4ccK3zaBBg+STTz6ROXPmmO137dolvXr18j2enZ1tQtTJkydl+fLlMn36dJk2bZo89dRTLh0VAAAAgFAX7ubO58+f73dbA5COKK1bt07atWsnR44ckXfeeUdmzpwp1113ndlm6tSp0rBhQxO+rrzySvn8889ly5YtsnDhQomLi5NmzZrJ6NGjZdiwYTJixAiJjIx06egAAAAAhCpXg1ReGpxU1apVzbUGKh2l6tChg2+bBg0aSEJCgqxYscIEKb1u0qSJCVFenTt3lnvvvVc2b94szZs3P2U/GRkZ5uJ19OhRc6370oubcnJyzHVUuEecMo6rbcmKKCMxMTESHe6RSNoSsO1xsy1RYY7fdSC9Lp5wj2mLvqfcfl8jMHj7Af0BgY6+imCSGYL9tbDH4nEcx/1Ppf8XIP70pz/J4cOH5auvvjL36UjUXXfd5Rd61BVXXCHXXnutjB07VgYMGCC//vqrfPbZZ77HU1NTpVy5cvLpp59K165dT9mXjlSNHDnylPt1f7rOCgAAAEDplJqaKn369DGDPBUrVgz8ESldK/Xdd9/5QtS5NHz4cBk8eLDfiFTt2rXN+qzTvVglYf369bJ7924ZNi9FnNi6rrblRNKXcnD+BInrM0Yi4y6kLQHaHjfboiNRo1vlyJNrwyQjxxNQr8vJvdtk78zHZNmyZdK0aVNX24LA+YZR1+N27NhRIiIi3G4OUCD6KoJJZgj2V+9stTMJiCB1//33y9y5c80Hnlq1avnuj4+PN0UkdJSqcuXKvvu1ap8+5t1m9erVfs/nrern3SavqKgoc8lL//hud4CwsN/rf2RkOeJke1xtS3pmtqSlpUk6bQno9gRCWzREZWR7AqItvjZlOaYt+p5y+32NwBII/9YDhUFfRTCJCKH+WtjjcLVqn84q1BD10UcfyeLFi6VuXf8RmJYtW5oDWbRoke8+LY+u5c7btGljbuv1pk2bZN++fb5tNBXryFKjRo1K8GgAAAAAlBbhbk/n03VJ//nPf8y5pPbs2WPur1Spklkkrtf9+/c30/C0AIWGowceeMCEJy00oXQ6ngam22+/XcaNG2ee44knnjDPnd+oEwAAAAAEdZCaNGmSub7mmmv87tcS53feeaf5+eWXXzZTc/REvFp0QivyvfHGG75ty5QpY6YFapU+DVhaZCIxMVFGjRpVwkcDAAAAoLRwNUgVpmBgdHS0TJw40VwKUqdOHVOhDwAAAABKgqtrpAAAAAAgGBGkAAAAAMASQQoAAAAALBGkAAAAAMASQQoAAAAALBGkAAAAAMASQQoAAAAALBGkAAAAAMASQQoAAAAALBGkAAAAAMASQQoAAAAALBGkAAAAAMASQQoAAAAALBGkAAAAAMASQQoAAAAALBGkAAAAAMASQQoAAAAALBGkAAAAAMASQQoAAAAALBGkAAAAAMASQQoAAAAALBGkAAAAAMBSuO0vAEAwSUpKkkBQrVo1SUhIcLsZAACgmBCkAISk7OOHRDwe6devnwSC6Jiykvx9EmEKAIAQQZACEJJyMo6LOI7EXj9EImJru9qWzAPb5cDc8bJ//36CFAAAIYIgBSCkaYiKiq/ndjMAAECIodgEAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAACAJYIUAAAAAFgiSAEAAABAMAWpZcuWyQ033CA1a9YUj8cjH3/8sd/jjuPIU089JTVq1JCYmBjp0KGDbN261W+bgwcPSt++faVixYpSuXJl6d+/vxw/fryEjwQAAABAaeJqkDpx4oQ0bdpUJk6cmO/j48aNk9dee00mT54sq1atknLlyknnzp0lPT3dt42GqM2bN8uCBQtk7ty5JpwNGDCgBI8CAAAAQGkT7ubOu3btai750dGoV155RZ544gnp0aOHuW/GjBkSFxdnRq5uvfVWSUpKkvnz58uaNWukVatWZpsJEyZIt27d5MUXXzQjXQAAAAAQUkHqdH7++WfZs2ePmc7nValSJWndurWsWLHCBCm91ul83hCldPuwsDAzgnXjjTfm+9wZGRnm4nX06FFznZmZaS5uysnJMddR4R5xyjiutiUrooyZUhkd7pFI2hKw7XGzLVFhjt81r0v+POEe0xb98sf7HndbbGys1KpVS0oT77/vbv87D5wJfRXBJDME+2thj8Xj6NBPANA1Uh999JH07NnT3F6+fLlcddVVsmvXLrNGyuvmm282277//vvy3HPPyfTp0yU5OdnvuapXry4jR46Ue++9N999jRgxwjye18yZM6Vs2bLFfmwAAAAAgkNqaqr06dNHjhw5YuowBN2I1Lk0fPhwGTx4sN+IVO3ataVTp06nfbFKwvr162X37t0ybF6KOLF1XW3LiaQv5eD8CRLXZ4xExl1IWwK0PW62RUeiRrfKkSfXhklGjofX5QxtqdrlAYmoer64LfPgTtMeXVOq61RLC/2GUdfTduzYUSIiItxuDlAg+iqCSWYI9lfvbLUzCdggFR8fb6737t3rNyKlt5s1a+bbZt++fX6/l5WVZSr5eX8/P1FRUeaSl/7x3e4AOi1RZWQ54mR7XG1Lema2pKWlSTptCej2BEJbNERlZHsCoi1egdiW7Io1JbzaReK27CzHtEf/vXH73zw3BMK/9UBh0FcRTCJCqL8W9jgC9jxSdevWNWFo0aJFfulQ1z61adPG3Nbrw4cPy7p163zbLF682KxB0LVUAAAAAHAuuDoiped7+vHHH/0KTGzYsEGqVq0qCQkJ8vDDD8szzzwj9evXN8HqySefNJX4vOuoGjZsKF26dJF77rnHlEjXocX777/fFKKgYh8AAACAkAxSa9eulWuvvdZ327tuKTExUaZNmyaPPvqoOdeUnhdKR57atm1ryp1HR0f7fue9994z4al9+/Zmmkrv3r3NuacAAAAAICSD1DXXXGPOF1UQrc43atQocymIjl5ptT0AAAAAKCkBu0YKAAAAAAIVQQoAAAAALBGkAAAAAMASQQoAAAAALBGkAAAAAMASQQoAAAAALBGkAAAAAMASQQoAAAAALBGkAAAAAMASQQoAAAAALBGkAAAAAMASQQoAAAAALBGkAAAAAMBSuO0vAABCQ1JSkgSCatWqSUJCgtvNAADACkEKAEqZ7OOHRDwe6devnwSC6Jiykvx9EmEKABBUCFIAUMrkZBwXcRyJvX6IRMTWdrUtmQe2y4G542X//v0EKQBAUCFIAUAppSEqKr6e280AACAoEaQAAKVivVZOTo653rhxo4SF5V9rifVaAIDCIkgBAErFeq2YmBiZNWuWtGvXTtLS0vLdhvVaAIDCIkgBAErFeq3ocI+5juszRtKznFMeZ70WAMAGQQoAUCrWa0WW0fCULZFxF4qT/Xuoyg9l4QEAhUGQAgCAsvAAAEsEKQAAKAsPALBEkAIAIBfKwgMACiP/+q8AAAAAgAIRpAAAAADAEkEKAAAAACwRpAAAAADAEkEKAAAAACwRpAAAAADAEkEKAAAAACwRpAAAAADAEkEKAAAAACwRpAAAAADAEkEKAAAAACwRpAAAAADAEkEKAAAAACyF2/4CAAAoGUlJSRIoqlWrJgkJCW43AwACBkEKAIAAk338kIjHI/369ZNAER1TVpK/TyJMAcD/IUgBABBgcjKOiziOxF4/RCJia7vdHMk8sF0OzB0v+/fvJ0gBwP8hSAEAEKA0REXF13O7GQCAfFBsAgAAAAAsEaQAAAAAwBJBCgAAAAAsEaQAAAAAwBJBCgAAAAAsEaQAAAAAwBLlzwEAQKEkJSVJIKhWrRrns8pHSkqKOddXoODvhFBHkAIAAKeVffyQiMcj/fr1k0AQFRUt//73v6RGjRpuNyVgwoKGqEsaNJT0tFQJFNExZSX5+6SAeH2Ac4EgBQAATisn47iI40js9UPMSYLdlL5jsxxe/A+5/vrrJVRDXU5OjrneuHGjhIWFFXq0UENUIPyNVOaB7XJg7ngzQkaQQqgiSAEAgELRD+hR8fVc/4Ae6qEuJiZGZs2aJe3atZO0tLSg+xsBpQVBCgAABJ1QDnXR4R5zHddnjKRnOYX6nbRta+XIl+8WWxsAnBlBCgAAIIBCXWQZDU/ZEhl3oTjZv4eqQoU6ACWK8ucAAAAAYIkgBQAAAACWCFIAAAAAYIkgBQAAAACWKDYBAACAc0LPbxUIMjIyJCoqSgJBoJzEGWePIAUAAIBilX38kIjHI/369ZOA4AkTcX4/0bHbomPKSvL3SYSpEECQAgAAQLHKyTgeMCdO9p5jKxDaomXqD8wdL/v37ydIhQCCFAAAAEL7xMkB0haElpAJUhMnTpQXXnhB9uzZI02bNpUJEybIFVdc4XazAAAAgICUkpJiRsfORk7O71MmN27cKGFhYaVq7VhIBKn3339fBg8eLJMnT5bWrVvLK6+8Ip07d5bk5GSpXr26280DAAAAAqoIx+7du6X3n2+SjPS0s3qemJgYmTVrlrRr107S0tJK1dqxkAhSL730ktxzzz1y1113mdsaqP73v//JlClT5LHHHnO7eQAAAEDgFeEQOeu1Y9HhHnMd12eMpGc5pWrtWNAHqZMnT8q6detk+PDhvvt0WLFDhw6yYsWKAktg6sXryJEj5vrgwYOSmZkpbjp69KikpqaK5+CvknMy3dW2hB3bLdHR0eI58LM4Of//9SrtbQm09rjZlpxwkdTU2pKze7s4WbwuwdCWQGtPSbYlb391sy1nEkhtCbT2lIa2nKmvlmRbiiqQ2hNIbZH9WyU6KkoqtPyTlKkQ62pTMvf+KCeSvpRIyZKIs3hdwnO0v6ZKeE66RBaxMKJHsszfSD8HHzhwQNx27Ngxc+04pw+GHudMWwS4Xbt2yfnnny/Lly+XNm3a+O5/9NFHZenSpbJq1apTfmfEiBEycuTIEm4pAAAAgGCxfft2qVWrVuiOSBWFjl7pmqrci+R0NCo2NlY8nt+HJ92iSbx27drmD1exYkVX2wKcCf0VwYT+imBBX0UwORqC/VXHmXRUqmbNmqfdLuiDlFb4KFOmjOzdu9fvfr0dHx+f7+/oma3znt26cuXKEki0I4ZKZ0Too78imNBfESzoqwgmFUOsv1aqVOmM2xS9RmGAiIyMlJYtW8qiRYv8Rpj0du6pfgAAAABQXIJ+RErpNL3ExERp1aqVOXeUlj8/ceKEr4ofAAAAABSnkAhSt9xyi/z222/y1FNPmRPyNmvWTObPny9xcXESbHTK4dNPP33K1EMgENFfEUzorwgW9FUEk6hS3F+DvmofAAAAAJS0oF8jBQAAAAAljSAFAAAAAJYIUgAAAABgiSAFAAAAAJYIUi54/vnn5fLLL5cKFSpI9erVpWfPnpKcnOy3TXp6ugwcOFBiY2OlfPny0rt371NOOgy4YcyYMeLxeOThhx/23Ud/RSDZuXOn9OvXz/THmJgYadKkiaxdu9b3uNZY0iqvNWrUMI936NBBtm7d6mqbUTplZ2fLk08+KXXr1jV98aKLLpLRo0ebPupFf4Vbli1bJjfccIPUrFnT/H//448/9nvcKUTfPHjwoPTt29ecqLdy5crSv39/OX78uIQKgpQLli5daj50rly5UhYsWCCZmZnSqVMnc+4rr0GDBsknn3wic+bMMdvv2rVLevXq5Wq7gTVr1sibb74pl112md/99FcEikOHDslVV10lERERMm/ePNmyZYuMHz9eqlSp4ttm3Lhx8tprr8nkyZNl1apVUq5cOencubP5QgAoSWPHjpVJkybJ66+/LklJSea29s8JEyb4tqG/wi36ubRp06YyceLEfB8fV4i+qSFq8+bN5vPu3LlzTTgbMGCAhAwtfw537du3T796cpYuXWpuHz582ImIiHDmzJnj2yYpKclss2LFChdbitLs2LFjTv369Z0FCxY4V199tfPQQw+Z++mvCCTDhg1z2rZtW+DjOTk5Tnx8vPPCCy/47tM+HBUV5cyaNauEWgn8rnv37s7dd9/td1+vXr2cvn37mp/prwgU+v/0jz76yHe7MH1zy5Yt5vfWrFnj22bevHmOx+Nxdu7c6YQCRqQCwJEjR8x11apVzfW6devMKJUOkXo1aNBAEhISZMWKFa61E6WbjqJ2797dr18q+isCyX//+19p1aqV3HTTTWbqdPPmzeXtt9/2Pf7zzz+bE7fn7q+VKlWS1q1b019R4v7whz/IokWL5IcffjC3N27cKF999ZV07drV3Ka/IlAVpm+uWLHCTOfTf5O9dPuwsDAzghUKwt1uQGmXk5Nj1proVJTGjRub+7RjRkZGms6XW1xcnHkMKGmzZ8+Wb775xkzty4v+ikCybds2M1Vq8ODB8ve//9302QcffND00cTERF+f1P6ZG/0Vbnjsscfk6NGj5sunMmXKmDVTzz77rJkOpeivCFSF6Zt79uwxX2jlFh4ebgYOQqX/EqQC4Fv+7777znwDBQSi7du3y0MPPWTmN0dHR7vdHOCMX07pt5/PPfecua0jUvpvrM7h1yAFBJIPPvhA3nvvPZk5c6ZceumlsmHDBvPlqi7up78CgY+pfS66//77zcK7JUuWSK1atXz3x8fHy8mTJ+Xw4cN+22sVNH0MKEk6dW/fvn3SokUL802SXrSghC4w1Z/12yf6KwKFVo9q1KiR330NGzaUlJQU87O3T+atKkl/hRuGDh1qRqVuvfVWU13y9ttvN8V7tLqvor8iUBWmb8bHx5vPD7llZWWZSn6h0n8JUi7QNXsaoj766CNZvHixKXuaW8uWLU3FKZ037aXl0fWDQJs2bVxoMUqz9u3by6ZNm8w3pd6LfuOvU0+8P9NfESh0mnTe00no+pM6deqYn/XfW/0feO7+qlOrdL4+/RUlLTU11awXyU2n+OnIqqK/IlAVpm+2adPGfMmqX8h66ede7d+6liokuF3tojS69957nUqVKjlffPGFs3v3bt8lNTXVt83f/vY3JyEhwVm8eLGzdu1ap02bNuYCBILcVfsU/RWBYvXq1U54eLjz7LPPOlu3bnXee+89p2zZss67777r22bMmDFO5cqVnf/85z/Ot99+6/To0cOpW7euk5aW5mrbUfokJiY6559/vjN37lzn559/dj788EOnWrVqzqOPPurbhv4KN6v1rl+/3lw0Mrz00kvm519//bXQfbNLly5O8+bNnVWrVjlfffWVqf572223OaGCIOUC7Yz5XaZOnerbRjvhfffd51SpUsV8CLjxxhtN2AICMUjRXxFIPvnkE6dx48amDG+DBg2ct956y+9xLdv75JNPOnFxcWab9u3bO8nJya61F6XX0aNHzb+l+kVUdHS0c+GFFzqPP/64k5GR4duG/gq3LFmyJN/Pq/oFQGH75oEDB0xwKl++vFOxYkXnrrvuMgEtVHj0P26PigEAAABAMGGNFAAAAABYIkgBAAAAgCWCFAAAAABYIkgBAAAAgCWCFAAAAABYIkgBAAAAgCWCFAAAAABYIkgBAAAAgCWCFACgVLnzzjulZ8+exf68e/bskY4dO0q5cuWkcuXKRXqOCy64QF555ZVibxsAoPgRpAAAQRNWbPzyyy/i8Xhkw4YNJbK/l19+WXbv3m3298MPP+S7zYgRI6RZs2YFPseaNWtkwIABhdofoQsA3BXu8v4BAAgJP/30k7Rs2VLq169f5Oc477zzirVNAIBzhxEpAECJ++6776Rr165Svnx5iYuLk9tvv13279/ve/yaa66RBx98UB599FGpWrWqxMfHm9Gc3L7//ntp27atREdHS6NGjWThwoVmBOrjjz82j9etW9dcN2/e3Nyvz5nbiy++KDVq1JDY2FgZOHCgZGZmnrbNkyZNkosuukgiIyPlkksukX/+859+o0P//ve/ZcaMGWZfOiJXFLlHmRzHMceckJAgUVFRUrNmTfOaeF+fX3/9VQYNGmT2pxcAQMkiSAEAStThw4fluuuuMwFn7dq1Mn/+fNm7d6/cfPPNfttNnz7drDdatWqVjBs3TkaNGiULFiwwj2VnZ5upg2XLljWPv/XWW/L444/7/f7q1avNtQYsnXL34Ycf+h5bsmSJGUHSa93PtGnTzKUgH330kTz00EMyZMgQEwL/+te/yl133WV+3zslr0uXLuYYdF+vvvrqWb9OGsx0uuCbb74pW7duNQGxSZMm5jE9llq1apnXRPenFwBAyWJqHwCgRL3++usmRD333HO++6ZMmSK1a9c2a4suvvhic99ll10mTz/9tPlZp8vp7y1atMgUdNBApUHoiy++MKNV6tlnnzWP5Z0mpyNO3m28qlSpYp6vTJky0qBBA+nevbt57nvuuSffNuvolY4y3Xfffeb24MGDZeXKleb+a6+91uxLR41iYmJO2VdRpaSkmOfq0KGDREREmJGpK664wjymo3Ta9goVKhTb/gAAdhiRAgCUqI0bN5qRHJ3W571omFEajrw0SOWm0/D27dtnfk5OTjbBK3eI8IaMwrj00ktNEMnvufOTlJQkV111ld99elvvP1duuukmSUtLkwsvvNAEPB0Vy8rKOmf7AwDYYUQKAFCijh8/LjfccIOMHTv2lMc00HjpKExuug4oJyenWNpwLp+7uGhQ1MCoUxN1BE5Hw1544QVZunTpKe0HAJQ8RqQAACWqRYsWsnnzZlNYoV69en4XXRNVGFrsYfv27WZtlZeuU8pNi0J411OdrYYNG8rXX3/td5/e1iIX55JOFdTQ+dprr5lpjCtWrJBNmzb5jq84jg0AUDSMSAEAzokjR46ccg4nb4W8t99+W2677TZfVb4ff/xRZs+eLf/4xz/8ptwVRNdCaQW9xMREU4ji2LFj8sQTT5jHvBXsqlevboKIFrPQwgxa3a9SpUpFOpahQ4eaQhK6tkvXLH3yySem4IOOFtnS6Xp5Xxdd66THk5sWv9Cg1Lp1a1NU49133zXHU6dOHfO4BtFly5bJrbfeatZnVatWrUjHBgAoGkakAADnhI6gaPDIfRk5cqQp462jORoSOnXqZCrRPfzww1K5cmUJCyvc/5Y0bGkVO50mePnll8tf/vIXX9U+DUwqPDzcjORo1TvdZ48ePYp8LFohUCvxaXEJXV+lzzl16tRTSqoXhhbUyPu6aBXAvPT10MCpa7F0vZiGNg1wGkaVVuzTkw5rAOP8UwBQ8jyOnqgCAIAgp+FMzyulo1t5R3cAAChuBCkAQFDSKnZa8U9Lo2t40vM8aVnzr776yu2mAQBKAdZIAQCCkq6LGjZsmDnfkq4P0rVL48ePd7tZAIBSghEpAAAAALBEsQkAAAAAsESQAgAAAABLBCkAAAAAsESQAgAAAABLBCkAAAAAsESQAgAAAABLBCkAAAAAsESQAgAAAACx8/8AkfqWLjv5MikAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"tmdb_5000_movies.csv\")\n",
        "\n",
        "# Remove rows with missing values in the 'overview' column\n",
        "df.dropna(subset=['overview'], inplace=True)\n",
        "\n",
        "# Apply to the DataFrame\n",
        "df['processed'] = df['overview'].apply(preprocess_text)\n",
        "\n",
        "# Create a dictionary and corpus for the processed text\n",
        "dictionary = Dictionary(df['processed'])\n",
        "\n",
        "# Map each list of tokens to a list of word indexes\n",
        "df['overview_indexed'] = df['processed'].apply(lambda tokens: dictionary.doc2idx(tokens))\n",
        "\n",
        "df['overview_length'] = df['overview_indexed'].apply(len)\n",
        "\n",
        "df = df[df['overview_length'] >= 20]\n",
        "\n",
        "# Create a mask column for the dataframe\n",
        "df = add_mask_column(df, 'overview_indexed', length=60, new_column_name=\"overview_masked\")\n",
        "\n",
        "# Plot the histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(df['overview_length'], bins=20, edgecolor='black')\n",
        "plt.title('Histogram of Overview List Lengths')\n",
        "plt.xlabel('Length of List')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "W = df[\"overview_indexed\"]\n",
        "Mask = df[\"overview_masked\"]\n",
        "rating = df[\"vote_average\"]\n",
        "rating = rating.round().astype(int)\n",
        "rating = torch.tensor(rating.values, dtype=torch.long)\n",
        "\n",
        "# Convert to a LongTensor (2D tensor)\n",
        "tensor_W = torch.tensor(W.tolist(), dtype=torch.long).T\n",
        "tensor_Mask = torch.tensor(Mask.tolist(), dtype=torch.bool).T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "30864eae",
      "metadata": {
        "id": "30864eae"
      },
      "outputs": [],
      "source": [
        "num_words = len(dictionary)\n",
        "num_topics = 2\n",
        "num_docs = W.shape[0]\n",
        "num_words_per_doc = 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "1f3dbba4",
      "metadata": {
        "id": "1f3dbba4"
      },
      "outputs": [],
      "source": [
        "def model(data=None, mask=None, batch_size=None, ratings=None, num_classes=None, device=\"cpu\"):\n",
        "    \"\"\"LDA model with masking for variable-length documents.\"\"\"\n",
        "    with pyro.plate(\"topics\", num_topics):\n",
        "        topic_words = pyro.sample(\n",
        "            \"topic_words\", dist.Dirichlet(0.1*torch.ones(num_words) / num_words)\n",
        "        )\n",
        "    \n",
        "    # Ordinal regression parameters (priors)\n",
        "    regression_coefs = None\n",
        "    cutpoints = None\n",
        "\n",
        "    if ratings is not None:\n",
        "        if num_classes is None:\n",
        "            raise ValueError(\"num_classes must be specified if ratings are provided.\")\n",
        "        if num_classes <= 1:\n",
        "            raise ValueError(\"num_classes must be greater than 1 for ordinal regression.\")\n",
        "\n",
        "        # Priors for regression coefficients (one per topic)\n",
        "        regression_coefs = pyro.sample(\n",
        "            \"regression_coefs\",\n",
        "            dist.Normal(torch.zeros(num_topics, device=device),\n",
        "                        torch.ones(num_topics, device=device)).to_event(1)\n",
        "        )\n",
        "\n",
        "        # Priors for cutpoints (num_classes - 1 cutpoints, ordered)\n",
        "        # c_0 ~ Normal\n",
        "        # c_i = c_{i-1} + exp(delta_i), where delta_i ~ Normal for i > 0\n",
        "        # This creates c_0 < c_1 < ... < c_{K-2}\n",
        "        \n",
        "        # Sample the first cutpoint\n",
        "        c_0 = pyro.sample(\"c_0\", \n",
        "                          dist.Normal(torch.tensor(0., device=device), \n",
        "                                      torch.tensor(5., device=device))) \n",
        "        \n",
        "        if num_classes > 2: # If more than one cutpoint is needed (i.e., num_classes - 1 > 1)\n",
        "            # Sample log differences for the remaining num_classes - 2 cutpoints\n",
        "            # These correspond to log(c_1 - c_0), log(c_2 - c_1), ...\n",
        "            log_diffs = pyro.sample(\n",
        "                \"log_diffs\",\n",
        "                dist.Normal(torch.zeros(num_classes - 2, device=device),\n",
        "                            torch.ones(num_classes - 2, device=device)).to_event(1)\n",
        "            )\n",
        "            # Calculate cutpoints: c_0, c_0 + exp(log_diffs_0), c_0 + exp(log_diffs_0) + exp(log_diffs_1), ...\n",
        "            # These are then c_0, c_1, c_2, ...\n",
        "            incremental_cutpoints = c_0 + torch.cumsum(torch.exp(log_diffs), dim=-1)\n",
        "            cutpoints = torch.cat([c_0.unsqueeze(0), incremental_cutpoints])\n",
        "        elif num_classes == 2: # Exactly one cutpoint: c_0\n",
        "            cutpoints = c_0.unsqueeze(0)\n",
        "        # If num_classes == 1, an error was raised earlier. No cutpoints needed.\n",
        "\n",
        "\n",
        "    with pyro.plate(\"documents\", num_docs, batch_size) as ind:\n",
        "        data = data[:, ind]  # data: [max_words, batch_size]\n",
        "        if mask is not None:\n",
        "            mask = mask[:, ind]  # same shape as data\n",
        "\n",
        "        ratings_batch = None\n",
        "        if ratings is not None: # ratings should be (num_docs,)\n",
        "            ratings_batch = ratings[ind]\n",
        "        \n",
        "        doc_topics = pyro.sample(\n",
        "            \"doc_topics\", dist.Dirichlet(0.1*torch.ones(num_topics) / num_topics)\n",
        "        )\n",
        "        \n",
        "        # Ordinal regression part\n",
        "        if ratings is not None and regression_coefs is not None and cutpoints is not None:\n",
        "            # regression_coefs shape: (num_topics)\n",
        "            # doc_topics shape: (batch_size, num_topics)\n",
        "            # eta = (doc_topics * regression_coefs.unsqueeze(0)).sum(dim=-1)\n",
        "            # Using matmul for clarity:\n",
        "            eta = torch.matmul(doc_topics, regression_coefs) # (batch_size, num_topics) @ (num_topics) -> (batch_size)\n",
        "            \n",
        "            pyro.sample(\n",
        "                \"observed_ratings\",\n",
        "                dist.OrderedLogistic(predictor=eta, cutpoints=cutpoints, validate_args=True),\n",
        "                obs=ratings_batch # ratings_batch should be (batch_size,) and contain integer classes 0..K-1\n",
        "            )\n",
        "\n",
        "        with pyro.plate(\"words\", num_words_per_doc):\n",
        "            with pyro.poutine.mask(mask=mask):\n",
        "                word_topics = pyro.sample(\n",
        "                    \"word_topics\", dist.Categorical(doc_topics),\n",
        "                    infer={\"enumerate\": \"parallel\"}\n",
        "                )\n",
        "\n",
        "                doc_words = pyro.sample(\n",
        "                    \"doc_words\",\n",
        "                    dist.Categorical(topic_words[word_topics]),\n",
        "                    obs=data\n",
        "                )\n",
        "\n",
        "\n",
        "    if ratings is not None:\n",
        "        return topic_words, doc_words, regression_coefs, cutpoints\n",
        "    else:\n",
        "        return topic_words, doc_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89f8cd28",
      "metadata": {
        "id": "89f8cd28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0] ELBO: 17481.3\n",
            "[100] ELBO: 1752735.2\n",
            "[200] ELBO: 1752499.9\n",
            "[300] ELBO: 1750553.4\n",
            "[400] ELBO: 1747548.1\n",
            "[500] ELBO: 1751113.0\n",
            "[600] ELBO: 1748153.2\n",
            "[700] ELBO: 1745700.9\n",
            "[800] ELBO: 1746321.1\n",
            "[900] ELBO: 1745952.7\n",
            "[1000] ELBO: 1747403.4\n",
            "[1100] ELBO: 1742516.8\n",
            "[1200] ELBO: 1742672.1\n",
            "[1300] ELBO: 1743200.8\n",
            "[1400] ELBO: 1743112.8\n",
            "[1500] ELBO: 1743161.2\n",
            "[1600] ELBO: 1740569.5\n",
            "[1700] ELBO: 1740207.0\n",
            "[1800] ELBO: 1744903.2\n",
            "[1900] ELBO: 1741170.6\n",
            "[2000] ELBO: 1740392.6\n",
            "[2100] ELBO: 1742727.8\n",
            "[2200] ELBO: 1741745.7\n",
            "[2300] ELBO: 1743489.9\n",
            "[2400] ELBO: 1748000.0\n",
            "[2500] ELBO: 1745694.7\n",
            "[2600] ELBO: 1750783.9\n",
            "[2700] ELBO: 1750846.2\n",
            "[2800] ELBO: 1752533.9\n",
            "[2900] ELBO: 1751536.4\n",
            "[3000] ELBO: 1756507.5\n",
            "[3100] ELBO: 1755535.9\n",
            "[3200] ELBO: 1759868.4\n",
            "[3300] ELBO: 1763713.4\n",
            "[3400] ELBO: 1765195.5\n",
            "[3500] ELBO: 1766884.7\n",
            "[3600] ELBO: 1770708.3\n",
            "[3700] ELBO: 1776282.2\n",
            "[3800] ELBO: 1780270.6\n",
            "[3900] ELBO: 1779567.2\n",
            "[4000] ELBO: 1779641.4\n",
            "[4100] ELBO: 1785968.6\n",
            "[4200] ELBO: 1790206.5\n",
            "[4300] ELBO: 1802261.3\n",
            "[4400] ELBO: 1796258.2\n",
            "[4500] ELBO: 1814506.2\n",
            "[4600] ELBO: 1809985.4\n",
            "[4700] ELBO: 1805312.8\n",
            "[4800] ELBO: 1813236.8\n",
            "[4900] ELBO: 1820092.5\n",
            "[5000] ELBO: 1821391.0\n",
            "[5100] ELBO: 1831110.8\n",
            "[5200] ELBO: 1837901.2\n",
            "[5300] ELBO: 1843920.3\n",
            "[5400] ELBO: 1847509.4\n"
          ]
        }
      ],
      "source": [
        "pyro.clear_param_store()\n",
        "\n",
        "def my_local_guide(data=None, mask=None, batch_size=None, ratings=None, num_classes=None, device=\"cpu\"): # Added ratings, num_classes, device\n",
        "    # Guide for topic_words\n",
        "    topic_words_posterior = pyro.param(\n",
        "            \"topic_words_posterior\", # Kept original name\n",
        "            lambda: torch.ones(num_topics, num_words, device=device), # Added device\n",
        "            constraint=constraints.positive)\n",
        "\n",
        "    with pyro.plate(\"topics\", num_topics):\n",
        "        pyro.sample(\"topic_words\", dist.Dirichlet(topic_words_posterior))\n",
        "\n",
        "    # Guide for ordinal regression parameters (if model includes them)\n",
        "    if ratings is not None:\n",
        "        if num_classes is None:\n",
        "            raise ValueError(\"num_classes must be specified for guide if ratings are provided.\")\n",
        "        if num_classes <= 1:\n",
        "            raise ValueError(\"num_classes must be greater than 1 for ordinal regression in guide.\")\n",
        "\n",
        "        # Guide for regression_coefs\n",
        "        q_regr_coefs_loc = pyro.param(\n",
        "            \"q_regr_coefs_loc\",\n",
        "            lambda: torch.zeros(num_topics, device=device))\n",
        "        q_regr_coefs_scale = pyro.param(\n",
        "            \"q_regr_coefs_scale\",\n",
        "            lambda: torch.ones(num_topics, device=device),\n",
        "            constraint=constraints.positive)\n",
        "        pyro.sample(\"regression_coefs\",\n",
        "                    dist.Normal(q_regr_coefs_loc, q_regr_coefs_scale).to_event(1))\n",
        "\n",
        "        # Guide for c_0\n",
        "        q_c0_loc = pyro.param(\n",
        "            \"q_c0_loc\",\n",
        "            lambda: torch.tensor(0.0, device=device))\n",
        "        q_c0_scale = pyro.param(\n",
        "            \"q_c0_scale\",\n",
        "            lambda: torch.tensor(1.0, device=device),\n",
        "            constraint=constraints.positive)\n",
        "        pyro.sample(\"c_0\", dist.Normal(q_c0_loc, q_c0_scale))\n",
        "\n",
        "        if num_classes > 2:\n",
        "            # Guide for log_diffs\n",
        "            q_log_diffs_loc = pyro.param(\n",
        "                \"q_log_diffs_loc\",\n",
        "                lambda: torch.zeros(num_classes - 2, device=device))\n",
        "            q_log_diffs_scale = pyro.param(\n",
        "                \"q_log_diffs_scale\",\n",
        "                lambda: torch.ones(num_classes - 2, device=device),\n",
        "                constraint=constraints.positive)\n",
        "            pyro.sample(\"log_diffs\",\n",
        "                        dist.Normal(q_log_diffs_loc, q_log_diffs_scale).to_event(1))\n",
        "\n",
        "    # Guide for doc_topics\n",
        "    doc_topics_posterior = pyro.param(\n",
        "            \"doc_topics_posterior\", # Kept original name\n",
        "            lambda: torch.ones(num_docs, num_topics, device=device), # Added device\n",
        "            constraint=constraints.positive) # Changed to positive for Dirichlet concentration\n",
        "\n",
        "    with pyro.plate(\"documents\", num_docs, batch_size) as ind:\n",
        "        # Sample from a Dirichlet distribution for doc_topics\n",
        "        pyro.sample(\"doc_topics\", dist.Dirichlet(doc_topics_posterior[ind]))\n",
        "\n",
        "\n",
        "if rating.min() > 0:\n",
        "    print(\"Warning: Ratings are not 0-indexed. Consider adjusting them or num_classes calculation.\")\n",
        "num_classes_val = rating.max().item() + 1 if rating.numel() > 0 else 0\n",
        "\n",
        "guide = AutoGuideList(model)\n",
        "guide.add(AutoDiagonalNormal(pyro.poutine.block(model, expose=['doc_topics']))) # We add the weights variables here\n",
        "guide.add(my_local_guide)  # automatically wrapped in an AutoCallable\n",
        "\n",
        "guide = my_local_guide\n",
        "\n",
        "elbo = TraceEnum_ELBO(max_plate_nesting=3)\n",
        "torch.autograd.set_detect_anomaly(True) \n",
        "optim = ClippedAdam({'lr': 0.0005, 'clip_norm': 1.0})\n",
        "svi = SVI(model, guide, optim, elbo)\n",
        "\n",
        "# Define the number of optimization steps\n",
        "n_steps = 10000\n",
        "device = \"cpu\"\n",
        "\n",
        "running_eblo = 0.0\n",
        "# do gradient steps\n",
        "for step in range(n_steps):\n",
        "    running_eblo += svi.step(tensor_W, tensor_Mask, ratings=None, num_classes=num_classes_val, device=device, batch_size=4)\n",
        "    if step % 100 == 0:\n",
        "        #print('.', end='')\n",
        "        print(\"[%d] ELBO: %.1f\" % (step, running_eblo/100))\n",
        "        running_eblo = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "a6056570",
      "metadata": {
        "id": "a6056570"
      },
      "outputs": [],
      "source": [
        "pyro.get_param_store().save(\"model_params.pyro\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "0ffca64e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic 1: said, year, us, firm, new, compani, last, govern, economi, market, rise, report, would, growth, say, sale, month, profit, cut, world, countri, one, product, three, bank, also, quarter, price, announc, oil, group, increas, share, could, figur, industri, expect, time, rose, invest, million, decemb, back, help, accord, financi, euro, demand, thursday, minist, biggest, car, busi, deal, earlier, foreign, trade, plan, econom, fall, mr, close, uk, two, friday, cost, forecast, financ, howev, fell, nation, unit, rais, tax, meet, account, rate, come, high, maker, giant, use, hope, lead, strong, record, set, gain, told, part, analyst, poor, money, number, job, feder, export, sinc, result, presid\n",
            "Topic 2: said, year, us, firm, new, govern, compani, last, economi, rise, market, report, would, growth, sale, say, month, cut, profit, world, countri, one, bank, three, product, also, industri, oil, quarter, increas, group, figur, share, invest, could, announc, price, expect, time, rose, accord, decemb, million, thursday, minist, financi, help, euro, biggest, demand, back, deal, car, busi, earlier, mr, trade, close, foreign, econom, plan, two, fall, uk, cost, financ, friday, hope, howev, unit, fell, tax, forecast, rais, high, account, giant, nation, rate, maker, meet, come, lead, set, analyst, use, job, number, feder, record, poor, money, part, result, gain, chief, sinc, strong, presid, offer\n",
            "Topic 3: said, year, firm, us, new, compani, last, govern, economi, market, rise, would, report, growth, sale, say, world, profit, cut, month, countri, one, bank, three, product, also, oil, price, group, industri, announc, could, quarter, time, share, invest, increas, expect, figur, back, million, decemb, thursday, accord, rose, car, minist, help, financi, euro, deal, busi, biggest, trade, earlier, demand, close, econom, foreign, plan, mr, cost, two, friday, fall, uk, unit, hope, financ, forecast, meet, maker, lead, account, nation, howev, tax, rais, fell, come, high, rate, analyst, use, giant, gain, set, result, chief, presid, record, make, strong, sinc, feder, poor, job, export, part, dollar\n",
            "Topic 4: said, year, firm, us, new, compani, last, govern, rise, economi, market, report, would, say, growth, cut, sale, profit, world, month, countri, one, also, product, bank, three, quarter, price, industri, announc, oil, share, time, group, increas, figur, could, invest, rose, expect, financi, decemb, back, minist, accord, million, help, car, earlier, euro, busi, deal, demand, foreign, trade, thursday, biggest, econom, two, uk, mr, friday, close, cost, fall, plan, fell, forecast, maker, financ, high, tax, nation, meet, howev, unit, rais, analyst, come, giant, account, rate, hope, use, set, told, lead, feder, number, job, gain, record, sinc, chief, estim, export, result, part, offer, hit\n",
            "Topic 5: said, year, us, firm, new, compani, last, govern, rise, market, economi, report, would, say, growth, sale, month, world, profit, cut, countri, product, one, three, bank, also, oil, quarter, price, share, group, figur, announc, industri, could, expect, time, increas, invest, decemb, back, rose, million, thursday, financi, help, accord, minist, deal, busi, car, euro, trade, close, foreign, two, demand, earlier, econom, cost, mr, biggest, plan, financ, friday, fall, unit, tax, uk, high, rais, maker, fell, howev, forecast, meet, account, nation, lead, giant, rate, come, hope, set, use, record, gain, number, strong, analyst, chief, result, presid, estim, feder, sinc, part, job, hit, told\n",
            "Topic 6: said, year, us, firm, new, compani, last, govern, economi, report, market, would, rise, growth, sale, say, world, month, cut, profit, countri, three, bank, product, one, also, share, increas, oil, could, industri, price, quarter, group, figur, announc, decemb, invest, time, million, expect, financi, rose, back, minist, help, car, euro, accord, thursday, busi, trade, demand, deal, earlier, close, biggest, plan, foreign, uk, fall, two, econom, cost, mr, friday, financ, maker, forecast, tax, high, fell, howev, giant, rate, lead, rais, account, meet, unit, record, set, nation, hope, use, analyst, come, hit, presid, chief, number, told, strong, gain, result, job, feder, sinc, money, part\n",
            "Topic 7: said, year, firm, us, new, compani, last, govern, report, rise, economi, market, would, say, growth, sale, month, profit, world, cut, countri, one, product, three, also, bank, oil, price, quarter, industri, announc, invest, increas, share, group, expect, million, time, rose, decemb, figur, could, thursday, financi, accord, deal, euro, help, demand, back, car, minist, trade, close, earlier, busi, foreign, mr, plan, fall, biggest, two, econom, uk, cost, forecast, maker, financ, rais, howev, account, unit, fell, high, meet, friday, nation, giant, rate, analyst, lead, tax, hope, told, strong, come, set, gain, use, job, record, chief, part, result, sinc, number, presid, feder, hit, offer\n",
            "Topic 8: said, year, us, firm, new, compani, last, govern, economi, rise, market, report, would, say, growth, sale, cut, month, world, profit, countri, three, bank, product, also, one, price, could, group, oil, share, time, figur, increas, invest, quarter, industri, expect, announc, financi, decemb, back, million, accord, minist, rose, thursday, car, deal, busi, demand, euro, help, econom, earlier, mr, plan, trade, close, biggest, foreign, uk, fall, two, friday, cost, financ, tax, forecast, fell, high, howev, rais, unit, maker, rate, hope, giant, analyst, come, meet, account, lead, use, gain, set, nation, sinc, record, strong, job, result, number, part, told, chief, poor, hit, estim, make\n",
            "Topic 9: said, year, us, firm, new, compani, last, govern, economi, market, rise, report, would, say, growth, sale, world, month, cut, countri, profit, one, bank, product, three, also, oil, quarter, industri, price, increas, share, figur, could, announc, invest, time, group, rose, expect, decemb, million, financi, accord, back, minist, euro, help, thursday, trade, busi, demand, deal, biggest, close, car, earlier, plan, foreign, friday, mr, econom, fall, two, uk, financ, forecast, cost, howev, meet, account, high, unit, tax, analyst, maker, rais, nation, giant, fell, rate, lead, hope, gain, come, set, use, strong, record, hit, presid, money, part, job, chief, poor, offer, feder, number, told\n",
            "Topic 10: said, year, us, firm, new, compani, last, govern, economi, market, rise, report, would, growth, say, sale, cut, month, world, profit, countri, one, also, three, product, bank, price, oil, quarter, industri, time, increas, figur, group, share, announc, invest, could, expect, rose, decemb, million, financi, help, thursday, trade, back, accord, car, earlier, euro, busi, minist, demand, deal, close, biggest, foreign, econom, plan, cost, two, mr, forecast, uk, fall, friday, financ, high, fell, unit, rate, lead, tax, howev, hope, maker, rais, analyst, account, nation, use, come, record, meet, strong, set, part, gain, giant, job, told, money, poor, hit, presid, result, number, feder, export\n"
          ]
        }
      ],
      "source": [
        "\n",
        "learned_topic_words = pyro.param(\"topic_words_posterior\").detach().cpu() # Or the specific name from your guide\n",
        "\n",
        "def get_top_words_for_topics(topic_words_dist, num_top_words=10):\n",
        "    top_words = {}\n",
        "    for i in range(topic_words_dist.shape[0]):  # Iterate over topics\n",
        "        top_word_indices = topic_words_dist[i].argsort(descending=True)[:num_top_words]\n",
        "        top_words[f\"Topic {i+1}\"] = [dictionary[idx.item()] for idx in top_word_indices]\n",
        "    return top_words\n",
        "\n",
        "# Assuming 'dictionary' is your gensim Dictionary object used for preprocessing\n",
        "top_words_per_topic = get_top_words_for_topics(learned_topic_words, num_top_words=100)\n",
        "\n",
        "for topic, words in top_words_per_topic.items():\n",
        "    print(f\"{topic}: {', '.join(words)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04b2f985",
      "metadata": {},
      "outputs": [],
      "source": [
        "[('find', 694),\n",
        " ('life', 657),\n",
        " ('one', 612),\n",
        " ('new', 582),\n",
        " ('world', 534),\n",
        " ('get', 532),\n",
        " ('live', 479),\n",
        " ('friend', 467),\n",
        " ('young', 457),\n",
        " ('take', 452),\n",
        " ('famili', 444),\n",
        " ('man', 435),\n",
        " ('becom', 432),\n",
        " ('year', 412),\n",
        " ('love', 411),\n",
        " ('two', 411),\n",
        " ('must', 372),\n",
        " ('stori', 363),\n",
        " ('make', 334),\n",
        " ('help', 322),\n",
        " ('time', 307),\n",
        " ('film', 307),"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "ee457831",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5, 14618])"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "learned_topic_words.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "7e73dce6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.return_types.sort(\n",
              "values=tensor([[5.6204e-05, 5.4016e-05, 3.4520e-05,  ..., 1.4539e-17, 1.3715e-17,\n",
              "         1.3143e-17],\n",
              "        [6.3218e-05, 4.8851e-05, 4.4877e-05,  ..., 1.3339e-17, 1.2775e-17,\n",
              "         1.1216e-17],\n",
              "        [8.7397e-05, 7.2012e-05, 3.8507e-05,  ..., 1.4976e-17, 1.3638e-17,\n",
              "         1.2633e-17],\n",
              "        [7.3586e-05, 6.2806e-05, 4.9497e-05,  ..., 1.2743e-17, 1.1234e-17,\n",
              "         1.0189e-17],\n",
              "        [7.0430e-05, 6.6849e-05, 4.9586e-05,  ..., 1.3644e-17, 1.3418e-17,\n",
              "         1.2536e-17]]),\n",
              "indices=tensor([[   25,   375,   228,  ..., 12781, 12316, 11803],\n",
              "        [  375,    81,    25,  ...,  7972,  9624, 14219],\n",
              "        [  375,    25,    81,  ..., 11704, 14226, 13071],\n",
              "        [  375,    25,    81,  ..., 10792, 14052,  8348],\n",
              "        [  375,   228,    25,  ..., 11977,  5713, 10592]]))"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "learned_topic_words.sort(descending=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e273545d",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "bml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
